{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a Gensim model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to use [`pyLDAvis`](https://github.com/bmabey/pyLDAvis)'s gensim [helper funtions](https://pyldavis.readthedocs.org/en/latest/modules/API.html#module-pyLDAvis.gensim) we will create a model from the [20 Newsgroup corpus](http://qwone.com/~jason/20Newsgroups/). Minimal preprocessing is done and so the model is not the best, the goal of this notebook is to demonstrate the the helper functions.\n",
    "\n",
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "pushd data\n",
    "if [ -d \"20news-bydate-train\" ]\n",
    "then\n",
    "  echo \"The data has already been downloaded...\"\n",
    "else\n",
    "  wget http://qwone.com/%7Ejason/20Newsgroups/20news-bydate.tar.gz\n",
    "  tar xfv 20news-bydate.tar.gz\n",
    "  rm 20news-bydate.tar.gz\n",
    "fi\n",
    "echo \"Lets take a look at the groups...\"\n",
    "ls 20news-bydate-train/\n",
    "popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group dir has a set of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 kostas kostas 1,5K Μάρ  18  2003 61250\r\n",
      "-rw------- 1 kostas kostas  889 Μάρ  18  2003 61252\r\n",
      "-rw------- 1 kostas kostas 1,2K Μάρ  18  2003 61264\r\n",
      "-rw------- 1 kostas kostas 1,7K Μάρ  18  2003 61308\r\n",
      "-rw------- 1 kostas kostas 1,4K Μάρ  18  2003 61422\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah data/20news-bydate-train/sci.space | tail  -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a peak at one email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ralph.buttigieg@f635.n713.z3.fido.zeta.org.au (Ralph Buttigieg)\r\n",
      "Subject: Why not give $1 billion to first year-lo\r\n",
      "Organization: Fidonet. Gate admin is fido@socs.uts.edu.au\r\n",
      "Lines: 34\r\n",
      "\r\n",
      "Original to: keithley@apple.com\r\n",
      "G'day keithley@apple.com\r\n",
      "\r\n",
      "21 Apr 93 22:25, keithley@apple.com wrote to All:\r\n",
      "\r\n",
      " kc> keithley@apple.com (Craig Keithley), via Kralizec 3:713/602\r\n",
      "\r\n",
      "\r\n",
      " kc> But back to the contest goals, there was a recent article in AW&ST\r\n",
      "about a\r\n",
      " kc> low cost (it's all relative...) manned return to the moon.  A General\r\n",
      " kc> Dynamics scheme involving a Titan IV & Shuttle to lift a Centaur upper\r\n",
      " kc> stage, LEV, and crew capsule.  The mission consists of delivering two\r\n",
      " kc> unmanned payloads to the lunar surface, followed by a manned mission.\r\n",
      " kc> Total cost:  US was $10-$13 billion.  Joint ESA(?)/NASA project was\r\n"
     ]
    }
   ],
   "source": [
    "!head data/20news-bydate-train/sci.space/61422 -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">alt.atheism</th>\n",
       "      <th>49960</th>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\n, Subject...</td>\n",
       "      <td>[from, mathew, #email, subject, alt, atheism, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51060</th>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\n, Subject...</td>\n",
       "      <td>[from, mathew, #email, subject, alt, atheism, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51119</th>\n",
       "      <td>[From: I3150101@dbstu1.rz.tu-bs.de (Benedikt R...</td>\n",
       "      <td>[from, #email, benedikt, rosenau, subject, gos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51120</th>\n",
       "      <td>[From: mathew &lt;mathew@mantis.co.uk&gt;\\n, Subject...</td>\n",
       "      <td>[from, mathew, #email, subject, university, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51121</th>\n",
       "      <td>[From: strom@Watson.Ibm.Com (Rob Strom)\\n, Sub...</td>\n",
       "      <td>[from, #email, rob, strom, subject, soc, motss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 doc  \\\n",
       "group       id                                                         \n",
       "alt.atheism 49960  [From: mathew <mathew@mantis.co.uk>\\n, Subject...   \n",
       "            51060  [From: mathew <mathew@mantis.co.uk>\\n, Subject...   \n",
       "            51119  [From: I3150101@dbstu1.rz.tu-bs.de (Benedikt R...   \n",
       "            51120  [From: mathew <mathew@mantis.co.uk>\\n, Subject...   \n",
       "            51121  [From: strom@Watson.Ibm.Com (Rob Strom)\\n, Sub...   \n",
       "\n",
       "                                                              tokens  \n",
       "group       id                                                        \n",
       "alt.atheism 49960  [from, mathew, #email, subject, alt, atheism, ...  \n",
       "            51060  [from, mathew, #email, subject, alt, atheism, ...  \n",
       "            51119  [from, #email, benedikt, rosenau, subject, gos...  \n",
       "            51120  [from, mathew, #email, subject, university, vi...  \n",
       "            51121  [from, #email, rob, strom, subject, soc, motss...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick and dirty....\n",
    "EMAIL_REGEX = re.compile(r\"[a-z0-9\\.\\+_-]+@[a-z0-9\\._-]+\\.[a-z]*\")\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX, \"#email\"), (FILTER_REGEX, ' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement, res)\n",
    "    return res.split()\n",
    "    \n",
    "def tokenize(lines, token_size_filter=2):\n",
    "    tokens = fp.mapcat(tokenize_line, lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "    \n",
    "\n",
    "def load_doc(filename):\n",
    "    group, doc_id = filename.split('/')[-2:]\n",
    "    with open(filename, 'r') as f:\n",
    "        doc = f.readlines()\n",
    "    return {'group': group,\n",
    "            'doc': doc,\n",
    "            'tokens': tokenize(doc),\n",
    "            'id': doc_id}\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(list(map(load_doc, glob('data/20news-bydate-train/*/*')))).set_index(['group','id'])\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dictionary, and bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "  print('Building dictionary...')\n",
    "  dictionary = Dictionary(docs)\n",
    "  #stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "  #stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "  #dictionary.filter_tokens(stopword_ids)\n",
    "  #dictionary.compactify()\n",
    "  #dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "  #dictionary.compactify()\n",
    "\n",
    "  print('Building corpus...')\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "  return dictionary, corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(docs['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize('newsgroups.mm', corpus)\n",
    "dictionary.save('newsgroups.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=10)\n",
    "                                      \n",
    "lda.save('newsgroups_50_lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model with pyLDAvis\n",
    "\n",
    "Okay, the moment we have all been waiting for is finally here!  You'll notice in the visualizaiton that we have a few junk topics that would probably disappear after better preprocessing of the corpus. This is left as an exercises to the reader. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-3b42f9a9b109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name gensim"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-91373969ed58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'topic.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "with open('topic.html', 'w+') as outf:\n",
    "    pyLDAvis.save_html(vis, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot sort an Index object in-place, use sort_values instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-72e664f93cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"topic_viz_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_passes_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pyLDAvis/gensim.pyc\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m    109\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pyLDAvis/_prepare.pyc\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m    \u001b[0mtopic_info\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m    \u001b[0mtoken_table\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m    \u001b[0mclient_topic_order\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_order\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pyLDAvis/_prepare.pyc\u001b[0m in \u001b[0;36m_token_table\u001b[1;34m(topic_info, term_topic_freq, vocab, term_frequency)\u001b[0m\n\u001b[0;32m    265\u001b[0m    \u001b[1;31m# term-topic frequency table of unique terms across all topics and all values of lambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m    \u001b[0mterm_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m    \u001b[0mterm_ix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m    \u001b[0mtop_topic_terms_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm_ix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m    \u001b[1;31m# use the new ordering for the topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36msort\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1733\u001b[1;33m         raise TypeError(\"cannot sort an Index object in-place, use \"\n\u001b[0m\u001b[0;32m   1734\u001b[0m                         \"sort_values instead\")\n\u001b[0;32m   1735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot sort an Index object in-place, use sort_values instead"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from gensim.models import VocabTransform\n",
    "\n",
    "# filter the dictionary\n",
    "old_dict = dictionary\n",
    "new_dict = copy.deepcopy(old_dict)\n",
    "new_dict.filter_extremes(keep_n=100000)\n",
    "new_dict.save('filtered.dict')\n",
    "\n",
    "# now transform the corpus\n",
    "corpus2 = corpus\n",
    "old2new = {old_dict.token2id[token]:new_id for new_id, token in new_dict.iteritems()}\n",
    "vt = VocabTransform(old2new)\n",
    "corpora.MmCorpus.serialize('filtered_corpus.mm', vt[corpus2], id2word=new_dict)\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, \"topic_viz_\"+topics+\"_passes_\"+passes+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the HDP model\n",
    "\n",
    "We could visualize the LDA model with pyLDAvis, in the same maner, we can also visualize gensim HDP models with pyLDAvis.\n",
    "\n",
    "The difference between HDP and LDA is that HDP is a non-parametric method. Which means you don't need to specify the number of topics, HDP will fit as many topics as it can and find the optimal number of topics by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The optional parameter T here indicates that HDP should find no more than 50 topics\n",
    "# if there exists any.\n",
    "hdp = models.hdpmodel.HdpModel(corpus, dictionary, T=50)\n",
    "                                      \n",
    "hdp.save('newsgroups_hdp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the HDP model with pyLDAvis\n",
    "\n",
    "As for the LDA model, you only need to give your model, the corpus and the dictionary associated to prepare the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(hdp, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "Gensim Newsgroup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
