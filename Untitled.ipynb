{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpath = '/media/kostas/DATA/GIT/NetworkTextVis/data/reddit.gexf'\n",
    "\n",
    "import networkx\n",
    "G = networkx.read_gexf(inpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:universal_logger:Node autowikibot could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SkarmNightmare could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SilverForUbers could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Arca9ine could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node hyperbolical could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node --Satan-- could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Lobo_Marino could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SomewhereDownInTexas could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node AdventIce could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node shnurg99 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node fieryscribe could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node bleebl00 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node andrewry could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Dane713 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node RhodesClosed could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node evilpenguin234 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node LuckyNickels could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node PhiladelphiaIrish could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node TweetsInCommentsBot could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node delatriangle could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Kyurem99XD could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node 150 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node VeryAwesome69 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node DNAnton could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Could not load: 24 users!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866\n"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "docs, list_docs = gather_texts(G, config, 'gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson.json_util import loads,dumps\n",
    "\n",
    "def gather_texts(graph,attribute,text_field):\n",
    "    objects = []\n",
    "    documents = []\n",
    "    list_docs = []\n",
    "    for i in list(graph.nodes()):\n",
    "        try:\n",
    "            #print graph.node[i][attribute]\n",
    "            objects.append(loads(graph.node[i][attribute],strict=False))\n",
    "        except Exception:\n",
    "            graph.remove_node(i)\n",
    "        #break\n",
    "    for o in objects:\n",
    "        posts = o[attribute]\n",
    "        concat_post = ''\n",
    "        tmp_doc = []\n",
    "        for p in posts:\n",
    "            concat_post = concat_post + ' ' + p[text_field]\n",
    "            tmp_doc.append(p[text_field])\n",
    "        documents.append(concat_post)\n",
    "        list_docs.append(tmp_doc)\n",
    "    print len(documents), len(list_docs)\n",
    "    return documents, list_docs\n",
    "\n",
    "docs, list_docs = gather_texts(G, 'posts','body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15202\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-f923ea44aa28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSafeConfigParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NetworkTextVis/config.ini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgather_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'edge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/media/kostas/DATA/GIT/NetworkTextVis/NetworkTextVis/network_load.pyc\u001b[0m in \u001b[0;36mgather_texts\u001b[1;34m(graph, config, scheme)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mconcat_post\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mtmp_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "docs, list_docs = gather_texts(G, config, 'edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "d = {'stop_words':'english', 'min_df':1, 'max_df':0.9}\n",
    "params = dict(config.items('vectorizer'))\n",
    "\n",
    "for key in params.keys():\n",
    "    if key == 'max_df':\n",
    "        params[key] = float(params[key])\n",
    "    if key == 'min_df':\n",
    "        params[key] = int(params[key])\n",
    "print params\n",
    "print d\n",
    "vectorizer = CountVectorizer(**params)\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import NetworkTextVis.text\n",
    "reload(NetworkTextVis.text)\n",
    "from NetworkTextVis.text import nfm_extraction\n",
    "\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "document_topic, document_topic_old, document_term, vocab, probable_words = nfm_extraction(docs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from topic_filtering import find_words_per_topic\n",
    "import NetworkTextVis.text\n",
    "reload(NetworkTextVis.text)\n",
    "from NetworkTextVis.text import find_words_per_topic\n",
    "topic_representations = find_words_per_topic(vocab, document_topic_old, document_term, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from NetworkTextVis.ranking import graph_influence\n",
    "clean_G = networkx.Graph()\n",
    "influence_dict = graph_influence(G)\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    #print node\n",
    "    tmp_td = document_topic[i]\n",
    "    tmp_tid = str(tmp_td.index(max(tmp_td)))\n",
    "    for topic_repr in topic_representations:\n",
    "        if topic_repr['topicId'] == 'topic_'+tmp_tid:\n",
    "            tmp_t = topic_repr['keywords']\n",
    "    clean_G.add_node(node, text=docs[i], list_texts=list_docs[i], label=node, topic_distr=tmp_td,\n",
    "                     topic_id = 'topic_'+tmp_tid)\n",
    "    words_str = ''\n",
    "    for cc,word in enumerate(tmp_t):\n",
    "        if cc == 0:\n",
    "            words_str = words_str + word\n",
    "        else:\n",
    "            words_str = words_str + ' ' + word\n",
    "    clean_G.node[node]['topic_words'] = words_str\n",
    "    clean_G.node[node]['topic_repr'] = ' '.join(tmp_t[0:3])\n",
    "    clean_G.node[node]['mean_infl'] = influence_dict[node]\n",
    "    #break\n",
    "for edge in G.edges():\n",
    "    clean_G.add_edge(*edge)\n",
    "clean_G.node[clean_G.nodes()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import topic_extraction\n",
    "reload(topic_extraction)\n",
    "import gensim\n",
    "reload(gensim)\n",
    "\n",
    "from topic_extraction import calculate_topic_distributions, nfm_extraction\n",
    "nfm_extraction(list_docs, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from argparse import ArgumentParser\n",
    "import networkx as nx\n",
    "from NetworkTextVis.network_load import read_network, gather_texts\n",
    "from NetworkTextVis.text import update_graph_topics\n",
    "from NetworkTextVis.ranking import graph_influence\n",
    "#from topic_extraction import gather_texts,topic_extract,graph_cleansing\n",
    "import urllib2\n",
    "import json\n",
    "import ConfigParser\n",
    "import logging\n",
    "import subprocess\n",
    "import webbrowser\n",
    "\n",
    "model = 'LDA'\n",
    "print G.nodes()\n",
    "docs, list_docs = gather_texts(G, config)\n",
    "G, out_G = update_graph_topics(G, docs, list_docs, config)\n",
    "print('Finished Topic Modeling!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "#G = nx.Graph([(1,2)])\n",
    "data = json_graph.node_link_data(G)\n",
    "s = json.dumps(data)\n",
    "with open('./data/graph.json', 'w+') as outfile:\n",
    "    json.dump(s, outfile)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/graph.json', 'r') as infile:\n",
    "    s1 = json.loads(infile)\n",
    "#G1 = networkx.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "def json_save(G, fname):\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()],\n",
    "                   edges=[[u, v, G.edge[u][v]] for u,v in G.edges()]),\n",
    "              fname, indent=2)\n",
    "\n",
    "def json_load(fname):\n",
    "    G = nx.DiGraph()\n",
    "    d = json.load(open(fname))\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    return G\n",
    "with open('./data/graph.json', 'w+') as outfile:\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()],\n",
    "                   edges=[[u, v, G.edge[u][v]] for u,v in G.edges()],\n",
    "                   directed = networkx.is_directed(G)), outfile)\n",
    "\n",
    "    \n",
    "import networkx\n",
    "def json_load(fname):\n",
    "    d = json.load(fname)\n",
    "    try:\n",
    "        if d['directed']==True:\n",
    "            G = networkx.DiGraph()\n",
    "        else:\n",
    "            G = networkx.Graph()\n",
    "    except KeyError:\n",
    "        G = networkx.Graph()\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    if networkx.is_directed(G):\n",
    "        type1 = 'directed'\n",
    "    else:\n",
    "        type1 = 'undirected'\n",
    "    return G\n",
    "with open('./data/graph.json', 'r') as infile:\n",
    "    GG = json_load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GG.node['oporotheca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/reddit.adjlist', 'w+') as infile:\n",
    "    networkx.write_adjlist(G,infile)\n",
    "    \n",
    "with open('./data/nodes.json', 'w+') as outfile:\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()]\n",
    "                   ), outfile)\n",
    "    \n",
    "def adjlist_load(adjlist_fname, node_fname):\n",
    "    G = networkx.read_adjlist(adjlist_fname)\n",
    "    nodes = json.load(node_fname)\n",
    "    G.add_nodes_from(nodes['nodes'])\n",
    "    return G\n",
    "with open('./data/reddit.adjlist', 'r') as adjlist_fname:\n",
    "    with open('./data/nodes.json') as node_fname:\n",
    "        GG = adjlist_load(adjlist_fname, node_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.loads(GG.node['LetoTheTyrant']['posts'])['posts'][0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/reddit.edgelist', 'w+') as outfile:\n",
    "    networkx.write_edgelist(G, outfile)\n",
    "\n",
    "def edgelist_load(edgelist_fname, node_fname):\n",
    "    G = networkx.read_edgelist(edgelist_fname)\n",
    "    nodes = json.load(node_fname)\n",
    "    print nodes['nodes']\n",
    "    #G.add_nodes_from(nodes['nodes'])\n",
    "    return G\n",
    "\n",
    "with open('./data/reddit.edgelist', 'r') as edgelist_fname:\n",
    "    with open('./data/nodes.json', 'r') as node_fname:\n",
    "        GG = edgelist_load(edgelist_fname, node_fname)\n",
    "    \n",
    "GG.node['LetoTheTyrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 0!\n",
      "23128\n"
     ]
    }
   ],
   "source": [
    "inpath = '/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json'\n",
    "import json, networkx\n",
    "\n",
    "data = []\n",
    "G = networkx.Graph()\n",
    "with open('/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json', 'r') as file_object:    # read all files\n",
    "    error_count = 0\n",
    "    for line in file_object:                        # read json.line\n",
    "        #print line\n",
    "        #print type(line)\n",
    "        #print json.loads(line)\n",
    "        #data.append(json.loads(line))\n",
    "        #break\n",
    "        try: \n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            data.append(tweet)\n",
    "        except:\n",
    "            print \"Error in tweets.json!\"           # error count\n",
    "            error_count += 1\n",
    "    print \"Total errors: %i!\" % error_count\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'indices': [3, 15], u'screen_name': u'KarlreMarks', u'id': 25058787, u'name': u'Karl Sharro', u'id_str': u'25058787'}]\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(data)):\n",
    "    if data[i]['entities']['user_mentions']:\n",
    "        print data[i]['entities']['user_mentions']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 0!\n",
      "23128\n"
     ]
    }
   ],
   "source": [
    "import networkx, json\n",
    "\n",
    "data = []\n",
    "G = networkx.Graph()\n",
    "with open('/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json', 'r') as file_object:    # read all files\n",
    "    error_count = 0\n",
    "    for line in file_object:                        # read json.line\n",
    "        try: \n",
    "            tweet = json.loads(line)\n",
    "            data.append(tweet)\n",
    "        except:\n",
    "            print \"Error in tweets.json!\"           # error count\n",
    "            error_count += 1\n",
    "    print \"Total errors: %i!\" % error_count\n",
    "print len(data)\n",
    "\n",
    "\n",
    "for i in xrange(len(data)):\n",
    "    cur_id = data[i]['user']['id']\n",
    "    if cur_id in G.nodes():\n",
    "        G.node[cur_id]['posts'].append({'tweet_id':data[i]['id'], 'text':data[i]['text']})\n",
    "    else:\n",
    "        G.add_node(cur_id,label=data[i]['user']['screen_name'], posts=[{ 'tweet_id':data[i]['id'], 'text':data[i]['text']}])\n",
    "    mentions = data[i]['entities']['user_mentions']\n",
    "    if mentions:\n",
    "        for mention in mentions:\n",
    "            if mention['id'] in G.nodes():\n",
    "                G.add_edge(cur_id, mention['id'])\n",
    "for node in G.nodes():\n",
    "    G.node[node]['posts'] = json.dumps(G.node[node]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15202\n"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts, read_network\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "scheme = 'twitter'\n",
    "#G = read_network(config, scheme)\n",
    "docs, list_docs = gather_texts(G, config, scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': u'NicoHines',\n",
       " 'posts': '[{\"tweet_id\": 547366774011080704, \"text\": \"RT @sunny_hundal: Hindu man in India caught desecrating temples &amp; writing pro-ISIS graffiti, so Muslims could be blamed http://t.co/muls9kf\\\\u2026\"}]'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node[G.nodes()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network', ' ne']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get('main', 'vis').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for posts in list_docs:\n",
    "    if type(posts) == list:\n",
    "        for post in posts:\n",
    "            l.append(post)\n",
    "    else:\n",
    "        l.append(posts)\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from NetworkTextVis.text import preprocess\n",
    "l = ['My name is Kostas', 'Kostas likes potatoes', 'My name name is Kostas Bougiatiotis']\n",
    "c = CountVectorizer()\n",
    "X = c.fit_transform(preprocess(l))\n",
    "#type(list_docs[0]) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bougiatiotis</th>\n",
       "      <th>is</th>\n",
       "      <th>kostas</th>\n",
       "      <th>likes</th>\n",
       "      <th>my</th>\n",
       "      <th>name</th>\n",
       "      <th>potatoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bougiatiotis  is  kostas  likes  my  name  potatoes\n",
       "0             0   1       1      0   1     1         0\n",
       "1             0   0       1      1   0     0         1\n",
       "2             1   1       1      0   1     2         0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = X.todense()\n",
    "a = pandas.DataFrame(gg, columns=c.get_feature_names())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "mask[0,0] = 1\n",
    "mask[0,1] = 1\n",
    "d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-a9cae0884bd5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-a9cae0884bd5>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    gg.reshape(d.shape).*d.reshape(gg.shape)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print numpy.array(gg)\n",
    "print numpy.array(d)\n",
    "gg.reshape(d.shape)*d.reshape(gg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 1.,  0.,  2.,  0.,  2.,  2.,  0.],\n",
       "       [ 1.,  2.,  0.,  1.,  2.,  2.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  1.],\n",
       "       [ 1.,  2.,  2.,  0.,  0.,  2.,  0.],\n",
       "       [ 1.,  2.,  2.,  1.,  2.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix = numpy.zeros([gg.shape[1], gg.shape[1]])\n",
    "words = c.get_feature_names()\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "for i in xrange(len(words)):\n",
    "    for j in xrange(len(words)):\n",
    "        mask[0,i] = 1\n",
    "        mask[0,j] = 1\n",
    "        #print words[i], words[j]\n",
    "        #print gg\n",
    "        #print 'MASK'\n",
    "        #print mask\n",
    "        d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "        #print 'RES'\n",
    "        #print (numpy.multiply(d, gg))\n",
    "        count_matrix[i, j] = int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))\n",
    "        #print count_matrix[i, j]\n",
    "        mask = numpy.zeros([1, gg.shape[1]])\n",
    "numpy.fill_diagonal(count_matrix, 0) \n",
    "count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd == count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 1.,  0.,  2.,  0.,  2.,  2.,  0.],\n",
       "       [ 1.,  2.,  0.,  1.,  2.,  2.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  1.],\n",
       "       [ 1.,  2.,  2.,  0.,  0.,  2.,  0.],\n",
       "       [ 1.,  2.,  2.,  1.,  2.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix = numpy.zeros([gg.shape[1], gg.shape[1]])\n",
    "words = c.get_feature_names()\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "for i in xrange(len(words)):\n",
    "    for j in xrange(i+1, len(words)):\n",
    "        mask[0,i] = 1\n",
    "        mask[0,j] = 1\n",
    "        #print words[i], words[j]\n",
    "        #print gg\n",
    "        #print 'MASK'\n",
    "        #print mask\n",
    "        d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "        #print 'RES'\n",
    "        #print (numpy.multiply(d, gg))\n",
    "        count_matrix[i, j] = int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))\n",
    "        #print count_matrix[i, j]\n",
    "        mask = numpy.zeros([1, gg.shape[1]])\n",
    "dd =count_matrix + count_matrix.T\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "aa = [['Obama', 'Putin'], ['Obama', 'Putin', 'Syria'], ['Syria', 'Trump']]\n",
    "dic = gensim.corpora.Dictionary(aa)\n",
    "corpus = [dic.doc2bow(ll) for ll in aa]\n",
    "gensim.matutils.corpus2dense(corpus, num_terms=len(dic.keys()), num_docs=len(aa)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy import English\n",
    "\n",
    "bb = ['Putin hates Obama', 'However Putin and Obama have a common foe in Syria',\n",
    "      'Syria also happens to be a trump card for Trump']\n",
    "nlp = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n"
     ]
    }
   ],
   "source": [
    "#named_entities = []\n",
    "#named_dic = {}\n",
    "#all_named_entities = {}\n",
    "#i = 0\n",
    "c = 0\n",
    "found_entities = []\n",
    "found_types = []\n",
    "counter = []\n",
    "#wanted_ents = ['LOC', 'ORG', 'PERSON', 'NORG', 'GPE']\n",
    "wanted_ents = ['ORG', 'PERSON','GPE']\n",
    "for posts in list_docs:\n",
    "    #print doc\n",
    "    for doc in posts:\n",
    "        ents = list(nlp(unicode(doc)).ents)\n",
    "        # if there are named entities\n",
    "        #tmp_ents = []\n",
    "        if ents:\n",
    "            for ent in ents:\n",
    "                if ent.label_ in wanted_ents:\n",
    "                    #tmp_ents.append(ent.orth_.encode('utf-8'))\n",
    "                    if not(ent.orth_.encode('utf-8') in found_entities):\n",
    "                        found_entities.append(ent.orth_.encode('utf-8'))\n",
    "                        found_types.append(ent.label_)\n",
    "                        counter.append(1)\n",
    "                    else:\n",
    "                        counter[found_entities.index(ent.orth_.encode('utf-8'))] += 1 \n",
    "                    # i += 1\n",
    "    #                 if  ent.label_ in named_dic.keys():\n",
    "    #                     named_dic[ent.label_].append(ent.orth_.encode('utf-8'))\n",
    "    #                 else:\n",
    "    #                     named_dic[ent.label_] = [ent.orth_.encode('utf-8')]\n",
    "                    # and if they are of the wanted type\n",
    "                    #if ent.label_ in wanted_ents:\n",
    "        #named_entities.append(tmp_ents)\n",
    "        c += 1\n",
    "        if c % 1000 == 0:\n",
    "            print c\n",
    "        \n",
    "#print named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'network, chord'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "config.get('main', 'vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "N_freq = 100\n",
    "freq_names = numpy.array(found_entities)[numpy.argsort(counter)[::-1][:N_freq]]\n",
    "freq_types = numpy.array(found_types)[numpy.argsort(counter)[::-1][:N_freq]]\n",
    "count_freq = numpy.zeros([N_freq, N_freq])\n",
    "for i, ent in enumerate(freq_names):\n",
    "    #print i\n",
    "    for j in xrange(i+1, len(freq_names)):\n",
    "        for doc in named_entities:\n",
    "            if ent in doc and freq_names[j] in doc:\n",
    "                count_freq[i,j] +=1\n",
    "numpy.fill_diagonal(count_freq, 0)\n",
    "count_freq += count_freq.T\n",
    "from sklearn.preprocessing import normalize\n",
    "count_freq /= count_freq.max()\n",
    "#count_freq = normalize(count_freq, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syria' 'ISIS' 'Iraq' 'RT' 'US' 'Iran' 'Aljazair Basmi Pimpinan Militan'\n",
      " 'Malaysia' 'U.S.' 'India' 'Damascus' 'Mesir' 'Palestin' 'News'\n",
      " 'Militer Aljazair' 'UN' 'FBI' 'ISIS. Mereka' 'U.S' 'Assad'\n",
      " 'Serangan Koalisi AS' 'Reuters' 'Islam' 'Aleppo' 'Idlib province' 'Isis'\n",
      " 'Israel' 'DANGEROUS STEPS' 'Memphis' 'Mum' 'Turkey' 'Algeria' 'State'\n",
      " 'U.N.' 'Isis Lee' 'ISIS.' 'Rough Rear Action' 'First Time In' 'Pakistan'\n",
      " 'Ferguson' 'Manhattan' 'YPG' 'Lebanon' 'Til Hamis' 'Penulis Jerman'\n",
      " 'Kobani' 'Egypt' 'Al-Maliki' 'Russia' 'Gaza' 'America' 'Kuwait'\n",
      " 'Amerika Serikat' 'Heritage Sites Damaged' 'CNN' 'Jurgen Todenhofer'\n",
      " 'Amnesty' 'Jerman Juergen Todenhoefer' 'Jack' 'France'\n",
      " 'Islamic State of Iraq' 'Kota Mosul' 'Jerman' 'Seorang' 'Jordan'\n",
      " 'Jordan Bahrain Morocco' 'Yezidi' \"Bashar Assad's\" 'Taliban' 'Haliburton'\n",
      " 'GWBush' 'Yarmouk' 'Obama' 'ISIS. Polisi' 'Hanya Arab'\n",
      " 'Juergen Todenhoefer' 'Homs' 'the Islamic State' 'North Korea'\n",
      " 'Indian Society' 'Qatar' 'Mosul' 'Raqqa' 'Fox' 'Secular and' 'Abu Quseyb'\n",
      " '&amp' 'J\\xc3\\xbcrgen Todenh\\xc3\\xb6fer' 'Mujeres y'\n",
      " 'DANGEROUS STEPS ISIS' 'Al-Azhar' 'Hamas' 'Lidia Herrera' 'ABC' 'Yazidi'\n",
      " 'Kota Bogor Bima Arya Sugiarto' 'Leia' 'Canada' 'Kobane' 'Douma']\n",
      "[u'GPE' u'ORG' u'GPE' u'ORG' u'GPE' u'GPE' u'ORG' u'GPE' u'GPE' u'GPE'\n",
      " u'GPE' u'PERSON' u'GPE' u'ORG' u'ORG' u'ORG' u'ORG' u'ORG' u'GPE'\n",
      " u'PERSON' u'ORG' u'ORG' u'ORG' u'GPE' u'GPE' u'PERSON' u'GPE' u'ORG'\n",
      " u'GPE' u'PERSON' u'GPE' u'GPE' u'ORG' u'ORG' u'PERSON' u'ORG' u'ORG'\n",
      " u'ORG' u'GPE' u'PERSON' u'GPE' u'ORG' u'GPE' u'GPE' u'ORG' u'PERSON'\n",
      " u'GPE' u'PERSON' u'GPE' u'GPE' u'GPE' u'GPE' u'PERSON' u'ORG' u'ORG'\n",
      " u'PERSON' u'ORG' u'PERSON' u'PERSON' u'GPE' u'ORG' u'PERSON' u'ORG' u'ORG'\n",
      " u'GPE' u'ORG' u'ORG' u'PERSON' u'ORG' u'GPE' u'PERSON' u'GPE' u'ORG'\n",
      " u'ORG' u'ORG' u'PERSON' u'GPE' u'ORG' u'GPE' u'ORG' u'GPE' u'GPE' u'GPE'\n",
      " u'ORG' u'ORG' u'GPE' u'ORG' u'PERSON' u'ORG' u'ORG' u'PERSON' u'ORG'\n",
      " u'PERSON' u'ORG' u'GPE' u'ORG' u'PERSON' u'GPE' u'ORG' u'GPE']\n",
      "[[ 0.          0.4478022   0.86630037 ...,  0.02472527  0.01007326\n",
      "   0.01831502]\n",
      " [ 0.4478022   0.          0.22985348 ...,  0.00641026  0.01556777\n",
      "   0.00274725]\n",
      " [ 0.86630037  0.22985348  0.         ...,  0.00732601  0.003663    0.0018315 ]\n",
      " ..., \n",
      " [ 0.02472527  0.00641026  0.00732601 ...,  0.          0.          0.        ]\n",
      " [ 0.01007326  0.01556777  0.003663   ...,  0.          0.          0.        ]\n",
      " [ 0.01831502  0.00274725  0.0018315  ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print freq_names\n",
    "print freq_types\n",
    "print count_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, ent in enumerate(found_entities):\n",
    "    #print i\n",
    "    for j in xrange(i+1, len(found_entities)):\n",
    "        for doc in named_entities:\n",
    "            if ent in doc and found_entities[j] in doc:\n",
    "                count_matrix[i,j] +=1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_ind = numpy.argsort(count_matrix.flatten())[::-1]\n",
    "rows, cols = numpy.unravel_index(sorted_ind[:N_freq], count_matrix.shape)\n",
    "count_freq = numpy.zeros([1, N_freq])\n",
    "for i in xrange(N_freq):\n",
    "    #print count_freq.shape\n",
    "    #print count_matrix[rows[i],cols[:N_freq]].shape\n",
    "    count_freq = numpy.vstack((count_freq, count_matrix[rows[i],cols[:N_freq]]))\n",
    "count_freq = count_freq[1:,:]\n",
    "freq_names = numpy.array(found_entities)[rows[:N_freq]]\n",
    "freq_types = numpy.array(found_types)[rows[:N_freq]]\n",
    "count_matrix = numpy.zeros([len(found_entities), len(found_entities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Dallas', 'Lions', 'Lions', 'Romo', 'Cowboys', 'Cowboys', 'Brady',\n",
       "        'Brady', 'Brady', 'Detroit', 'Lindley', 'Stafford', 'Brady',\n",
       "        'Brady', 'Lions', 'Brady', 'Dallas', 'Lions', 'Brady', 'Brady',\n",
       "        'NFL', 'Lions', 'Romo', 'Rodgers', 'NFL', 'Brady', 'Dallas',\n",
       "        'Detroit', 'Cowboys', 'Romo', 'Arizona', 'Brady', 'Lions', 'Murray',\n",
       "        'Lions', 'Lindley', 'Arizona', 'Rodgers', 'Dallas', 'Brady',\n",
       "        'Dallas', 'Brady', 'Lions', 'Cowboys', 'Brady', 'Dallas', 'Arizona',\n",
       "        'Brady', 'Witten', 'Dallas', 'Dallas', 'Lindley', 'Rodgers',\n",
       "        'Brady', 'Romo', 'Rodgers', 'Fox', 'Romo', 'Rodgers', 'Cowboys',\n",
       "        'Cowboys', 'Cowboys', 'Lions', 'Stafford', 'Detroit', 'Lions',\n",
       "        'Cowboys', 'Lions', 'Dallas', 'Cowboys', 'Brady', 'Murray',\n",
       "        'Murray', 'Ravens', 'Lions', 'Romo', 'Lions', 'Cowboys', 'Joe',\n",
       "        'Joe', 'Arizona', 'NFL', 'Joe', 'NFL', 'Seattle', 'Seattle', 'PI',\n",
       "        'Brady', 'Brady', 'PI', 'Brady', 'NFL', 'Stafford', 'Romo',\n",
       "        'Dallas', 'Cowboys', 'Cowboys', 'Ravens', 'Lions', 'Rodgers'], \n",
       "       dtype='|S85'),\n",
       " array([u'GPE', u'ORG', u'ORG', u'PERSON', u'ORG', u'ORG', u'PERSON',\n",
       "        u'PERSON', u'PERSON', u'GPE', u'PERSON', u'PERSON', u'PERSON',\n",
       "        u'PERSON', u'ORG', u'PERSON', u'GPE', u'ORG', u'PERSON', u'PERSON',\n",
       "        u'ORG', u'ORG', u'PERSON', u'PERSON', u'ORG', u'PERSON', u'GPE',\n",
       "        u'GPE', u'ORG', u'PERSON', u'GPE', u'PERSON', u'ORG', u'PERSON',\n",
       "        u'ORG', u'PERSON', u'GPE', u'PERSON', u'GPE', u'PERSON', u'GPE',\n",
       "        u'PERSON', u'ORG', u'ORG', u'PERSON', u'GPE', u'GPE', u'PERSON',\n",
       "        u'PERSON', u'GPE', u'GPE', u'PERSON', u'PERSON', u'PERSON',\n",
       "        u'PERSON', u'PERSON', u'ORG', u'PERSON', u'PERSON', u'ORG', u'ORG',\n",
       "        u'ORG', u'ORG', u'PERSON', u'GPE', u'ORG', u'ORG', u'ORG', u'GPE',\n",
       "        u'ORG', u'PERSON', u'PERSON', u'PERSON', u'PERSON', u'ORG',\n",
       "        u'PERSON', u'ORG', u'ORG', u'PERSON', u'PERSON', u'GPE', u'ORG',\n",
       "        u'PERSON', u'ORG', u'GPE', u'GPE', u'ORG', u'PERSON', u'PERSON',\n",
       "        u'ORG', u'PERSON', u'ORG', u'PERSON', u'PERSON', u'GPE', u'ORG',\n",
       "        u'ORG', u'PERSON', u'ORG', u'PERSON'], \n",
       "       dtype='<U6'))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_names, freq_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('ne.json', 'w+') as outf:\n",
    "    json.dump(named_dic, outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "dic = gensim.corpora.Dictionary(named_entities)\n",
    "corpus = [dic.doc2bow(ll) for ll in named_entities]\n",
    "gg = gensim.matutils.corpus2dense(corpus, num_terms=len(dic.keys()), num_docs=len(named_entities)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3179 3179\n",
      "(866, 3179)\n"
     ]
    }
   ],
   "source": [
    "print len(found_types), len(found_entities)\n",
    "print gg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866 3179\n",
      "(866, 3179)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-155472ad908b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#print 'RES'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print (numpy.multiply(d, gg))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mcount_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m#print count_matrix[i, j]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print len(docs), len(found_entities)\n",
    "print gg.shape\n",
    "count_matrix = numpy.zeros([gg.shape[1], gg.shape[1]])\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "for i in xrange(gg.shape[1]):\n",
    "    print i\n",
    "    for j in xrange(i+1, gg.shape[1]):\n",
    "        # print j\n",
    "        mask[0,i] = 1\n",
    "        mask[0,j] = 1\n",
    "        #print words[i], words[j]\n",
    "        #print gg\n",
    "        #print 'MASK'\n",
    "        #print mask\n",
    "        d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "        #print 'RES'\n",
    "        #print (numpy.multiply(d, gg))\n",
    "        count_matrix[i, j] = int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))\n",
    "        #print count_matrix[i, j]\n",
    "        mask = numpy.zeros([1, gg.shape[1]])\n",
    "numpy.fill_diagonal(count_matrix, 0)\n",
    "count_matrix += count_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 2657\n"
     ]
    }
   ],
   "source": [
    "print i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv, json\n",
    "wanted_ents = ['ORG', 'PERSON',  'GPE']\n",
    "color_dic = ['#4CA54C', '#e32636', '#5d8aa8'] #['#ffbc42', '#8f2d56', '#73d2de']\n",
    "names_list = [['name', 'color']]\n",
    "for i, ent in enumerate(freq_names):\n",
    "    names_list.append([ent, color_dic[wanted_ents.index(freq_types[i])]])\n",
    "    \n",
    "with open('./visualization1_Chord_Diagram/Data/names.csv', 'w+') as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerows(names_list)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "#normalize(count_freq, norm='l1', axis=1)\n",
    "b = count_freq.tolist()\n",
    "with open('./visualization1_Chord_Diagram/Data/chord_matrix.json', 'w+') as out_json:\n",
    "    json.dump(count_freq.tolist(), out_json)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
