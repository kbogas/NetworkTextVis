{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpath = '/media/kostas/DATA/GIT/NetworkTextVis/data/reddit.gexf'\n",
    "\n",
    "import networkx\n",
    "G = networkx.read_gexf(inpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:universal_logger:Node autowikibot could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SkarmNightmare could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SilverForUbers could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Arca9ine could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node hyperbolical could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node --Satan-- could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Lobo_Marino could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node SomewhereDownInTexas could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node AdventIce could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node shnurg99 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node fieryscribe could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node bleebl00 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node andrewry could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Dane713 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node RhodesClosed could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node evilpenguin234 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node LuckyNickels could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node PhiladelphiaIrish could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node TweetsInCommentsBot could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node delatriangle could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node Kyurem99XD could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node 150 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node VeryAwesome69 could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Node DNAnton could not gather texts, due to parsing errors\n",
      "WARNING:universal_logger:Could not load: 24 users!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866\n"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "docs, list_docs = gather_texts(G, config, 'gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson.json_util import loads,dumps\n",
    "\n",
    "def gather_texts(graph,attribute,text_field):\n",
    "    objects = []\n",
    "    documents = []\n",
    "    list_docs = []\n",
    "    for i in list(graph.nodes()):\n",
    "        try:\n",
    "            #print graph.node[i][attribute]\n",
    "            objects.append(loads(graph.node[i][attribute],strict=False))\n",
    "        except Exception:\n",
    "            graph.remove_node(i)\n",
    "        #break\n",
    "    for o in objects:\n",
    "        posts = o[attribute]\n",
    "        concat_post = ''\n",
    "        tmp_doc = []\n",
    "        for p in posts:\n",
    "            concat_post = concat_post + ' ' + p[text_field]\n",
    "            tmp_doc.append(p[text_field])\n",
    "        documents.append(concat_post)\n",
    "        list_docs.append(tmp_doc)\n",
    "    print len(documents), len(list_docs)\n",
    "    return documents, list_docs\n",
    "\n",
    "docs, list_docs = gather_texts(G, 'posts','body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15202\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-f923ea44aa28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSafeConfigParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NetworkTextVis/config.ini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgather_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'edge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/media/kostas/DATA/GIT/NetworkTextVis/NetworkTextVis/network_load.pyc\u001b[0m in \u001b[0;36mgather_texts\u001b[1;34m(graph, config, scheme)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mconcat_post\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mtmp_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "docs, list_docs = gather_texts(G, config, 'edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "d = {'stop_words':'english', 'min_df':1, 'max_df':0.9}\n",
    "params = dict(config.items('vectorizer'))\n",
    "\n",
    "for key in params.keys():\n",
    "    if key == 'max_df':\n",
    "        params[key] = float(params[key])\n",
    "    if key == 'min_df':\n",
    "        params[key] = int(params[key])\n",
    "print params\n",
    "print d\n",
    "vectorizer = CountVectorizer(**params)\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import NetworkTextVis.text\n",
    "reload(NetworkTextVis.text)\n",
    "from NetworkTextVis.text import nfm_extraction\n",
    "\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "document_topic, document_topic_old, document_term, vocab, probable_words = nfm_extraction(docs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from topic_filtering import find_words_per_topic\n",
    "import NetworkTextVis.text\n",
    "reload(NetworkTextVis.text)\n",
    "from NetworkTextVis.text import find_words_per_topic\n",
    "topic_representations = find_words_per_topic(vocab, document_topic_old, document_term, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from NetworkTextVis.ranking import graph_influence\n",
    "clean_G = networkx.Graph()\n",
    "influence_dict = graph_influence(G)\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    #print node\n",
    "    tmp_td = document_topic[i]\n",
    "    tmp_tid = str(tmp_td.index(max(tmp_td)))\n",
    "    for topic_repr in topic_representations:\n",
    "        if topic_repr['topicId'] == 'topic_'+tmp_tid:\n",
    "            tmp_t = topic_repr['keywords']\n",
    "    clean_G.add_node(node, text=docs[i], list_texts=list_docs[i], label=node, topic_distr=tmp_td,\n",
    "                     topic_id = 'topic_'+tmp_tid)\n",
    "    words_str = ''\n",
    "    for cc,word in enumerate(tmp_t):\n",
    "        if cc == 0:\n",
    "            words_str = words_str + word\n",
    "        else:\n",
    "            words_str = words_str + ' ' + word\n",
    "    clean_G.node[node]['topic_words'] = words_str\n",
    "    clean_G.node[node]['topic_repr'] = ' '.join(tmp_t[0:3])\n",
    "    clean_G.node[node]['mean_infl'] = influence_dict[node]\n",
    "    #break\n",
    "for edge in G.edges():\n",
    "    clean_G.add_edge(*edge)\n",
    "clean_G.node[clean_G.nodes()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import topic_extraction\n",
    "reload(topic_extraction)\n",
    "import gensim\n",
    "reload(gensim)\n",
    "\n",
    "from topic_extraction import calculate_topic_distributions, nfm_extraction\n",
    "nfm_extraction(list_docs, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from argparse import ArgumentParser\n",
    "import networkx as nx\n",
    "from NetworkTextVis.network_load import read_network, gather_texts\n",
    "from NetworkTextVis.text import update_graph_topics\n",
    "from NetworkTextVis.ranking import graph_influence\n",
    "#from topic_extraction import gather_texts,topic_extract,graph_cleansing\n",
    "import urllib2\n",
    "import json\n",
    "import ConfigParser\n",
    "import logging\n",
    "import subprocess\n",
    "import webbrowser\n",
    "\n",
    "model = 'LDA'\n",
    "print G.nodes()\n",
    "docs, list_docs = gather_texts(G, config)\n",
    "G, out_G = update_graph_topics(G, docs, list_docs, config)\n",
    "print('Finished Topic Modeling!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "#G = nx.Graph([(1,2)])\n",
    "data = json_graph.node_link_data(G)\n",
    "s = json.dumps(data)\n",
    "with open('./data/graph.json', 'w+') as outfile:\n",
    "    json.dump(s, outfile)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/graph.json', 'r') as infile:\n",
    "    s1 = json.loads(infile)\n",
    "#G1 = networkx.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "def json_save(G, fname):\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()],\n",
    "                   edges=[[u, v, G.edge[u][v]] for u,v in G.edges()]),\n",
    "              fname, indent=2)\n",
    "\n",
    "def json_load(fname):\n",
    "    G = nx.DiGraph()\n",
    "    d = json.load(open(fname))\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    return G\n",
    "with open('./data/graph.json', 'w+') as outfile:\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()],\n",
    "                   edges=[[u, v, G.edge[u][v]] for u,v in G.edges()],\n",
    "                   directed = networkx.is_directed(G)), outfile)\n",
    "\n",
    "    \n",
    "import networkx\n",
    "def json_load(fname):\n",
    "    d = json.load(fname)\n",
    "    try:\n",
    "        if d['directed']==True:\n",
    "            G = networkx.DiGraph()\n",
    "        else:\n",
    "            G = networkx.Graph()\n",
    "    except KeyError:\n",
    "        G = networkx.Graph()\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    if networkx.is_directed(G):\n",
    "        type1 = 'directed'\n",
    "    else:\n",
    "        type1 = 'undirected'\n",
    "    return G\n",
    "with open('./data/graph.json', 'r') as infile:\n",
    "    GG = json_load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GG.node['oporotheca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/reddit.adjlist', 'w+') as infile:\n",
    "    networkx.write_adjlist(G,infile)\n",
    "    \n",
    "with open('./data/nodes.json', 'w+') as outfile:\n",
    "    json.dump(dict(nodes=[[n, G.node[n]] for n in G.nodes()]\n",
    "                   ), outfile)\n",
    "    \n",
    "def adjlist_load(adjlist_fname, node_fname):\n",
    "    G = networkx.read_adjlist(adjlist_fname)\n",
    "    nodes = json.load(node_fname)\n",
    "    G.add_nodes_from(nodes['nodes'])\n",
    "    return G\n",
    "with open('./data/reddit.adjlist', 'r') as adjlist_fname:\n",
    "    with open('./data/nodes.json') as node_fname:\n",
    "        GG = adjlist_load(adjlist_fname, node_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.loads(GG.node['LetoTheTyrant']['posts'])['posts'][0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/reddit.edgelist', 'w+') as outfile:\n",
    "    networkx.write_edgelist(G, outfile)\n",
    "\n",
    "def edgelist_load(edgelist_fname, node_fname):\n",
    "    G = networkx.read_edgelist(edgelist_fname)\n",
    "    nodes = json.load(node_fname)\n",
    "    print nodes['nodes']\n",
    "    #G.add_nodes_from(nodes['nodes'])\n",
    "    return G\n",
    "\n",
    "with open('./data/reddit.edgelist', 'r') as edgelist_fname:\n",
    "    with open('./data/nodes.json', 'r') as node_fname:\n",
    "        GG = edgelist_load(edgelist_fname, node_fname)\n",
    "    \n",
    "GG.node['LetoTheTyrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 0!\n",
      "23128\n"
     ]
    }
   ],
   "source": [
    "inpath = '/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json'\n",
    "import json, networkx\n",
    "\n",
    "data = []\n",
    "G = networkx.Graph()\n",
    "with open('/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json', 'r') as file_object:    # read all files\n",
    "    error_count = 0\n",
    "    for line in file_object:                        # read json.line\n",
    "        #print line\n",
    "        #print type(line)\n",
    "        #print json.loads(line)\n",
    "        #data.append(json.loads(line))\n",
    "        #break\n",
    "        try: \n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            data.append(tweet)\n",
    "        except:\n",
    "            print \"Error in tweets.json!\"           # error count\n",
    "            error_count += 1\n",
    "    print \"Total errors: %i!\" % error_count\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'indices': [3, 15], u'screen_name': u'KarlreMarks', u'id': 25058787, u'name': u'Karl Sharro', u'id_str': u'25058787'}]\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(data)):\n",
    "    if data[i]['entities']['user_mentions']:\n",
    "        print data[i]['entities']['user_mentions']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 0!\n",
      "23128\n"
     ]
    }
   ],
   "source": [
    "import networkx, json\n",
    "\n",
    "data = []\n",
    "G = networkx.Graph()\n",
    "with open('/media/kostas/DATA/GIT/NetworkTextVis/data/twitter.json', 'r') as file_object:    # read all files\n",
    "    error_count = 0\n",
    "    for line in file_object:                        # read json.line\n",
    "        try: \n",
    "            tweet = json.loads(line)\n",
    "            data.append(tweet)\n",
    "        except:\n",
    "            print \"Error in tweets.json!\"           # error count\n",
    "            error_count += 1\n",
    "    print \"Total errors: %i!\" % error_count\n",
    "print len(data)\n",
    "\n",
    "\n",
    "for i in xrange(len(data)):\n",
    "    cur_id = data[i]['user']['id']\n",
    "    if cur_id in G.nodes():\n",
    "        G.node[cur_id]['posts'].append({'tweet_id':data[i]['id'], 'text':data[i]['text']})\n",
    "    else:\n",
    "        G.add_node(cur_id,label=data[i]['user']['screen_name'], posts=[{ 'tweet_id':data[i]['id'], 'text':data[i]['text']}])\n",
    "    mentions = data[i]['entities']['user_mentions']\n",
    "    if mentions:\n",
    "        for mention in mentions:\n",
    "            if mention['id'] in G.nodes():\n",
    "                G.add_edge(cur_id, mention['id'])\n",
    "for node in G.nodes():\n",
    "    G.node[node]['posts'] = json.dumps(G.node[node]['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ConfigParser\n",
    "import NetworkTextVis.network_load\n",
    "reload(NetworkTextVis.network_load)\n",
    "from NetworkTextVis.network_load import gather_texts, read_network\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "config.read('NetworkTextVis/config.ini')\n",
    "#docs, list_docs = gather_texts(G, config)\n",
    "scheme = 'twitter'\n",
    "#G = read_network(config, scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': u'NicoHines',\n",
       " 'posts': '[{\"tweet_id\": 547366774011080704, \"text\": \"RT @sunny_hundal: Hindu man in India caught desecrating temples &amp; writing pro-ISIS graffiti, so Muslims could be blamed http://t.co/muls9kf\\\\u2026\"}]'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node[G.nodes()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network', ' ne']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get('main', 'vis').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for posts in list_docs:\n",
    "    if type(posts) == list:\n",
    "        for post in posts:\n",
    "            l.append(post)\n",
    "    else:\n",
    "        l.append(posts)\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from NetworkTextVis.text import preprocess\n",
    "l = ['My name is Kostas', 'Kostas likes potatoes', 'My name name is Kostas Bougiatiotis']\n",
    "c = CountVectorizer()\n",
    "X = c.fit_transform(preprocess(l))\n",
    "#type(list_docs[0]) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg = X.todense()\n",
    "a = pandas.DataFrame(gg, columns=c.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "mask[0,0] = 1\n",
    "mask[0,1] = 1\n",
    "d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 1.,  0.,  2.,  0.,  2.,  2.,  0.],\n",
       "       [ 1.,  2.,  0.,  1.,  2.,  2.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  1.],\n",
       "       [ 1.,  2.,  2.,  0.,  0.,  2.,  0.],\n",
       "       [ 1.,  2.,  2.,  1.,  2.,  1.,  1.],\n",
       "       [ 0.,  0.,  1.,  1.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix = numpy.zeros([gg.shape[1], gg.shape[1]])\n",
    "words = c.get_feature_names()\n",
    "mask = numpy.zeros([1, gg.shape[1]])\n",
    "for i in xrange(len(words)):\n",
    "    for j in xrange(len(words)):\n",
    "        mask[0,i] = 1\n",
    "        mask[0,j] = 1\n",
    "        #print words[i], words[j]\n",
    "        #print gg\n",
    "        #print 'MASK'\n",
    "        #print mask\n",
    "        d = numpy.repeat(mask, gg.shape[0], 0)\n",
    "        #print 'RES'\n",
    "        #print (numpy.multiply(d, gg))\n",
    "        count_matrix[i, j] = int(sum(numpy.multiply(d, gg).sum(axis=1)>=2))\n",
    "        #print count_matrix[i, j]\n",
    "        mask = numpy.zeros([1, gg.shape[1]])\n",
    "count_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "aa = [['Obama', 'Putin'], ['Obama', 'Putin', 'Syria'], ['Syria', 'Trump']]\n",
    "dic = gensim.corpora.Dictionary(aa)\n",
    "corpus = [dic.doc2bow(ll) for ll in aa]\n",
    "gensim.matutils.corpus2dense(corpus, num_terms=len(dic.keys()), num_docs=len(aa)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3, 1]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
